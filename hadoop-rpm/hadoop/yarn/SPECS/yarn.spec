# rpmrebuild autogenerated specfile

%define defaultbuildroot /
AutoProv: no
%undefine __find_provides
AutoReq: no
%undefine __find_requires
# Do not try autogenerate prereq/conflicts/obsoletes and check files
%undefine __check_files
%undefine __find_prereq
%undefine __find_conflicts
%undefine __find_obsoletes
# Be sure buildpolicy set to do nothing
%define __spec_install_post %{nil}
# Something that need for rpm-4.1
%define _missing_doc_files_terminate_build 0

# HDP specific parameters
%define hdp_version 2.6.1.0
%define hadoop_src /root/hadoop
%define hadoop_version 3.1.1-SNAPSHOT

BuildArch:     x86_64
Name:          hadoop_2_6_1_0_129-yarn
Version:       2.7.3.2.6.1.0
Release:       129
License:       Apache License v2.0 
Group:         System/Daemons
Summary:       The Hadoop NextGen MapReduce (YARN)


URL:           http://hadoop.apache.org/core/







Provides:      hadoop_2_6_1_0_129-yarn = 2.7.3.2.6.1.0-129
Provides:      hadoop_2_6_1_0_129-yarn(x86-64) = 2.7.3.2.6.1.0-129
Requires:      hadoop_2_6_1_0_129 = 2.7.3.2.6.1.0-129
Requires:      /bin/sh  
#Requires:      rpmlib(FileDigests) <= 4.6.0-1
#Requires:      rpmlib(PayloadFilesHavePrefix) <= 4.0-1
#Requires:      rpmlib(CompressedFileNames) <= 3.0.4-1
Requires:      /bin/bash  
Requires:      /usr/bin/env  
Requires:      libc.so.6()(64bit)  
Requires:      libc.so.6(GLIBC_2.2.5)(64bit)  
Requires:      libc.so.6(GLIBC_2.3)(64bit)  
#Requires:      rpmlib(PayloadIsXz) <= 5.2-1
#suggest
#enhance
%description
YARN (Hadoop NextGen MapReduce) is a general purpose data-computation framework.
The fundamental idea of YARN is to split up the two major functionalities of the
JobTracker, resource management and job scheduling/monitoring, into separate daemons:
ResourceManager and NodeManager.

The ResourceManager is the ultimate authority that arbitrates resources among all
the applications in the system. The NodeManager is a per-node slave managing allocation
of computational resources on a single node. Both work in support of per-application
ApplicationMaster (AM).

An ApplicationMaster is, in effect, a framework specific library and is tasked with
negotiating resources from the ResourceManager and working with the NodeManager(s) to
execute and monitor the tasks.
%prep -p /bin/sh
# make sure that the current build dir is configured under /usr/hdp

echo "Build root: $RPM_BUILD_ROOT"
echo "HDP version: %{hdp_version}"
echo "Release: %{release}"
echo "Hadoop src: %{hadoop_src}"
echo "Hadoop version: %{hadoop_version}"

%define hdp_build %hdp_version-%release
%define hdp /usr/hdp/%hdp_version-%release
%define jar_version %{hadoop_version}.%{hdp_build}
HADOOP_DIST=%hadoop_src/hadoop-dist/target/hadoop-%hadoop_version

if [ ! -d "$RPM_BUILD_ROOT" ]; then
  echo "Build root $RPM_BUILD_ROOT doesn't exist"
  mkdir $RPM_BUILD_ROOT
fi

if [ ! -d "$HADOOP_DIST" ]; then
  echo "Hadoop distribution source doesn't exist. Make sure that a version of Hadoop is built from source and is available under hadoop-dist module at $HADOOP_DIST"
  exit -1;
fi

# cleanup everything before we start
rm -rf $RPM_BUILD_ROOT/*

# create a current HDP dir
mkdir -p $RPM_BUILD_ROOT%{hdp}

# current build parameters
HDP_OUT=$RPM_BUILD_ROOT%hdp
OUT=$RPM_BUILD_ROOT%hdp/hadoop-yarn
DIST=$HADOOP_DIST
SRC=%hadoop_src

# hadoop/conf.empty
mkdir -p $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/capacity-scheduler.xml $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/container-executor.cfg $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/yarn-env.sh $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/yarn-site.xml $HDP_OUT/etc/hadoop/conf.empty

# yarn security limits
mkdir -p $HDP_OUT/etc/security/limits.d
cat > $HDP_OUT/etc/security/limits.d/yarn.conf <<"EOL"
yarn   - nofile 32768
yarn   - nproc  65536
EOL

# hadoop-yarn/conf symlink
mkdir -p $OUT
pushd $OUT; ln -s /etc/hadoop/conf conf; popd

# hadoop-yarn/bin
mkdir -p $OUT/bin
cp $DIST/bin/container-executor $OUT/bin
cp $DIST/bin/container-executor $OUT/bin
cp $DIST/bin/yarn $OUT/bin/yarn.distro
sed -i 's/HADOOP_SHELL_EXECNAME="\${MYNAME\#\#\*\/}"/HADOOP_SHELL_EXECNAME="yarn"/g' $OUT/bin/yarn.distro

cat > $OUT/bin/mapred <<"EOL"
export HADOOP_HOME=${HADOOP_HOME:-$hdp/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-$hdp/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-$hdp/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"
export YARN_OPTS="${YARN_OPTS} -Dhdp.version=${HDP_VERSION}"

exec $hdp/hadoop-mapreduce/bin/mapred.distro "$@"
EOL

cat > $OUT/bin/yarn <<"EOL"
#!/bin/bash

export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"
export YARN_OPTS="${YARN_OPTS} -Dhdp.version=${HDP_VERSION}"

exec %{hdp}/hadoop-yarn/bin/yarn.distro "$@"
EOL

# hadoop/bin/yarn
mkdir -p $HDP_OUT/hadoop/bin
cat > $HDP_OUT/hadoop/bin/yarn <<"EOL"
#!/bin/bash

export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"
export YARN_OPTS="${YARN_OPTS} -Dhdp.version=${HDP_VERSION}"

exec %{hdp}/hadoop-yarn/bin/yarn.distro "$@"
EOL

mkdir -p $OUT/etc
pushd $OUT/etc; ln -s ../../hadoop/conf hadoop; popd

# hadoop-yarn common jars
declare -A cj=(
  [hadoop-yarn-api]=share/hadoop/yarn/hadoop-yarn-api-%{hadoop_version}.jar
  [hadoop-yarn-client]=share/hadoop/yarn/hadoop-yarn-client-%{hadoop_version}.jar
  [hadoop-yarn-common]=share/hadoop/yarn/hadoop-yarn-common-%{hadoop_version}.jar
  [hadoop-yarn-registry]=share/hadoop/yarn/hadoop-yarn-registry-%{hadoop_version}.jar
  [hadoop-yarn-applications-distributedshell]=share/hadoop/yarn/hadoop-yarn-applications-distributedshell-%{hadoop_version}.jar
  [hadoop-yarn-applications-unmanaged-am-launcher]=share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-%{hadoop_version}.jar
  [hadoop-yarn-server-applicationhistoryservice]=share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-%{hadoop_version}.jar
  [hadoop-yarn-server-common]=share/hadoop/yarn/hadoop-yarn-server-common-%{hadoop_version}.jar
  [hadoop-yarn-server-router]=share/hadoop/yarn/hadoop-yarn-server-router-%{hadoop_version}.jar
  [hadoop-yarn-server-nodemanager]=share/hadoop/yarn/hadoop-yarn-server-nodemanager-%{hadoop_version}.jar
  [hadoop-yarn-server-resourcemanager]=share/hadoop/yarn/hadoop-yarn-server-resourcemanager-%{hadoop_version}.jar
  [hadoop-yarn-server-sharedcachemanager]=share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-%{hadoop_version}.jar
  [hadoop-yarn-server-tests]=share/hadoop/yarn/hadoop-yarn-server-tests-%{hadoop_version}.jar
  [hadoop-yarn-server-timeline-pluginstorage]=share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-%{hadoop_version}.jar
  [hadoop-yarn-server-web-proxy]=share/hadoop/yarn/hadoop-yarn-server-web-proxy-%{hadoop_version}.jar
  [hadoop-yarn-services-core]=share/hadoop/yarn/hadoop-yarn-services-core-%{hadoop_version}.jar
  [hadoop-yarn-services-api]=share/hadoop/yarn/hadoop-yarn-services-api-%{hadoop_version}.jar
)
pushd $OUT
for f in "${!cj[@]}"; do
  p=${cj[${f}]}
  cp $DIST/$p $f-%{jar_version}.jar
  ln -s $f-%{jar_version}.jar $f.jar
done
popd
# copy the new timelineservice 
cp -r $DIST/share/hadoop/yarn/timelineservice $OUT

# hadoop-yarn/lib
cp -r $DIST/share/hadoop/yarn/lib $OUT
cp -rf $DIST/share/hadoop/common/lib/*.jar $OUT/lib

# hadoop-yarn/sbin
mkdir -p $OUT/sbin
cp -r $DIST/sbin/yarn-daemon.sh $OUT/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/sbin/yarn-daemon.sh

cp -r $DIST/sbin/yarn-daemons.sh $OUT/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/sbin/yarn-daemons.sh

# hadoop/libexec/yarn-config.sh
mkdir -p $HDP_OUT/hadoop/libexec
cp -r $DIST/libexec/yarn-config.sh $HDP_OUT/hadoop/libexec

%files
# default attribues
%defattr(0644, root, root, 0755)

# yarn config
%config(noreplace) %attr(0644, root, root) "%hdp/etc/hadoop/conf.empty"
%config(noreplace) %attr(0644, root, root) "%hdp/etc/security/limits.d"

# everything else
%attr(0755, root, root) "%hdp/hadoop-yarn/*"
%attr(0755, root, root) "%hdp/hadoop/*"
%pre -p /bin/sh
getent group yarn >/dev/null || groupadd -r yarn
getent passwd yarn >/dev/null || /usr/sbin/useradd --comment "Hadoop Yarn" --shell /bin/bash -M -r -g yarn -G hadoop --home /var/lib/hadoop-yarn yarn

if [[ ! -e "/var/log/hadoop-yarn" ]]; then
    /usr/bin/install -d -o yarn -g hadoop -m 0775  /var/log/hadoop-yarn
fi

if [[ ! -e "/var/run/hadoop-yarn" ]]; then
    /usr/bin/install -d -o yarn -g hadoop -m 0775  /var/run/hadoop-yarn
fi

if [[ ! -e "/var/lib/hadoop-yarn" ]]; then
    /usr/bin/install -d -o yarn -g hadoop -m 0755  /var/lib/hadoop-yarn
fi

if [[ ! -e "/var/lib/hadoop-yarn/cache" ]]; then
    /usr/bin/install -d -o yarn -g hadoop -m 1777  /var/lib/hadoop-yarn/cache
fi
%changelog
