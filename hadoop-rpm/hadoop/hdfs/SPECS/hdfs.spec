# rpmrebuild autogenerated specfile

%define defaultbuildroot /
AutoProv: no
%undefine __find_provides
AutoReq: no
%undefine __find_requires
# Do not try autogenerate prereq/conflicts/obsoletes and check files
%undefine __check_files
%undefine __find_prereq
%undefine __find_conflicts
%undefine __find_obsoletes
# Be sure buildpolicy set to do nothing
%define __spec_install_post %{nil}
# Something that need for rpm-4.1
%define _missing_doc_files_terminate_build 0

# HDP specific parameters
%define hdp_version 2.6.1.0
%define hadoop_src /root/hadoop
%define hadoop_version 3.0.2-SNAPSHOT

BuildArch:     x86_64
Name:          hadoop_2_6_1_0_129-hdfs
Version:       2.7.3.2.6.1.0
Release:       129
License:       Apache License v2.0 
Group:         System/Daemons
Summary:       The Hadoop Distributed File System


URL:           http://hadoop.apache.org/core/







Provides:      hadoop_2_6_1_0_129-hdfs = 2.7.3.2.6.1.0-129
Provides:      hadoop_2_6_1_0_129-hdfs(x86-64) = 2.7.3.2.6.1.0-129
Requires:      hadoop_2_6_1_0_129 = 2.7.3.2.6.1.0-129
Requires:      bigtop-jsvc  
Requires:      libtirpc-devel  
Requires:      /bin/sh  
#Requires:      rpmlib(FileDigests) <= 4.6.0-1
#Requires:      rpmlib(PayloadFilesHavePrefix) <= 4.0-1
#Requires:      rpmlib(CompressedFileNames) <= 3.0.4-1
Requires:      /bin/bash  
Requires:      /bin/sh  
Requires:      /usr/bin/env  
#Requires:      rpmlib(PayloadIsXz) <= 5.2-1
#suggest
#enhance
%description
Hadoop Distributed File System (HDFS) is the primary storage system used by
Hadoop applications. HDFS creates multiple replicas of data blocks and distributes
them on compute nodes throughout a cluster to enable reliable, extremely rapid
computations.
%prep -p /bin/sh
# make sure that the current build dir is configured under /usr/hdp

echo "Build root: $RPM_BUILD_ROOT"
echo "HDP version: %{hdp_version}"
echo "Release: %{release}"
echo "Hadoop src: %{hadoop_src}"
echo "Hadoop version: %{hadoop_version}"

%define hdp_build %hdp_version-%release
%define hdp /usr/hdp/%hdp_version-%release
%define jar_version %{hadoop_version}.%{hdp_build}
HADOOP_DIST=%hadoop_src/hadoop-dist/target/hadoop-%hadoop_version

if [ ! -d "$RPM_BUILD_ROOT" ]; then
  echo "Build root $RPM_BUILD_ROOT doesn't exist"
  mkdir -p $RPM_BUILD_ROOT
fi

if [ ! -d "$HADOOP_DIST" ]; then
  echo "Hadoop distribution source doesn't exist. Make sure that a version of Hadoop is built from source and is available under hadoop-dist module at $HADOOP_DIST"
  exit -1;
fi

# cleanup everything before we start
rm -rf $RPM_BUILD_ROOT/*

# create a current HDP dir
mkdir -p $RPM_BUILD_ROOT%{hdp}

# copy required hadoop only files from HADOOP_SRC to BUILDROOT
HDP_OUT=$RPM_BUILD_ROOT%hdp
OUT=$RPM_BUILD_ROOT%hdp/hadoop-hdfs
DIST=$HADOOP_DIST
SRC=%hadoop_src

# hadoop conf.empty for hdfs
mkdir -p $HDP_OUT/etc/hadoop/conf.empty
cp $DIST/etc/hadoop/hdfs-site.xml $HDP_OUT/etc/hadoop/conf.empty

# hdfs security limits
mkdir -p $HDP_OUT/etc/security/limits.d
cat > $HDP_OUT/etc/security/limits.d/hdfs.conf <<"EOL"
hdfs   - nofile 128000
hdfs   - nproc  65536
EOL

# hadoop-hdfs/bin
mkdir -p $OUT/bin
cp $DIST/bin/hdfs $OUT/bin/hdfs.distro
sed -i 's/HADOOP_SHELL_EXECNAME="\${MYNAME\#\#\*\/}"/HADOOP_SHELL_EXECNAME="hdfs"/g' $OUT/bin/hdfs.distro

# setup HDP specific hdfs shell
cat > $OUT/bin/hdfs <<"EOL"
#!/bin/bash

export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"

exec %{hdp}/hadoop-hdfs/bin/hdfs.distro "$@"
EOL

# etc/rc.d file. don't know what's it for
mkdir -p $OUT/etc/rc.d/init.d

# hadoop-hdfs common jars
declare -A cj=(
  [hadoop-hdfs]=share/hadoop/hdfs/hadoop-hdfs-%{hadoop_version}.jar
  [hadoop-hdfs-tests]=share/hadoop/hdfs/hadoop-hdfs-%{hadoop_version}-tests.jar
  [hadoop-hdfs-nfs]=share/hadoop/hdfs/hadoop-hdfs-nfs-%{hadoop_version}.jar
  [hadoop-hdfs-native-client]=share/hadoop/hdfs/hadoop-hdfs-native-client-%{hadoop_version}.jar
  [hadoop-hdfs-native-client-tests]=share/hadoop/hdfs/hadoop-hdfs-native-client-%{hadoop_version}-tests.jar
  [hadoop-hdfs-httpfs]=share/hadoop/hdfs/hadoop-hdfs-httpfs-%{hadoop_version}.jar
  [hadoop-hdfs-client]=share/hadoop/hdfs/hadoop-hdfs-client-%{hadoop_version}.jar
  [hadoop-hdfs-client-tests]=share/hadoop/hdfs/hadoop-hdfs-client-%{hadoop_version}-tests.jar
)
pushd $OUT
for f in "${!cj[@]}"; do
  p=${cj[${f}]}
  cp $DIST/$p $f-%{jar_version}.jar
  ln -s $f-%{jar_version}.jar $f.jar
done
popd

# hadoop-hdfs/lib
cp -r $DIST/share/hadoop/hdfs/lib $OUT/

# hadoop-hdfs/webapp
cp -r $DIST/share/hadoop/hdfs/webapps $OUT/

# hadoop-hdfs/sbin
mkdir -p $OUT/sbin
cp $DIST/sbin/distribute-exclude.sh $OUT/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/sbin/distribute-exclude.sh

cp $DIST/sbin/refresh-namenodes.sh $OUT/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/sbin/refresh-namenodes.sh

# hadoop/libexec
mkdir -p $HDP_OUT/hadoop/libexec
cp $DIST/libexec/hdfs-config.sh $HDP_OUT/hadoop/libexec

# script to initialize hdfs
cat > $HDP_OUT/hadoop/libexec/init-hdfs.sh <<"EOL"
#!/bin/bash -ex

# Use this script to initialize HDFS directory structure for various components to run. This script can be run from any node in the Hadoop cluster but should only be run once by one node only. If you are planning on using oozie, we recommend that you run this script from a node that has hive, pig, sqoop, etc. installed. Unless you are using psuedo distributed cluster, this node is most likely NOT your namenode
# Steps to be performed before running this script:
# 1. Stop the namenode and datanode services if running.
# 2. Format namenode (su -s /bin/bash hdfs hdfs namenode -format).
# 3. Start the namenode and datanode services on appropriate nodes.
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /tmp'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 1777 /tmp'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /var'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /var/log'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 1775 /var/log'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown yarn:mapred /var/log'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /tmp/hadoop-yarn'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /tmp/hadoop-yarn'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir -p /var/log/hadoop-yarn/apps'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 1777 /var/log/hadoop-yarn/apps'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown yarn:mapred /var/log/hadoop-yarn/apps'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /hbase'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown hbase:hbase /hbase'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /solr'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown solr:solr /solr'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /benchmarks'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /benchmarks'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod 755 /user'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown hdfs  /user'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/history'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown mapred:mapred /user/history'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod 755 /user/history'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/jenkins'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /user/jenkins'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown jenkins /user/jenkins'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/hive'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /user/hive'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown hive /user/hive'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/root'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /user/root'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown root /user/root'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/hue'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /user/hue'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown hue /user/hue'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/sqoop'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /user/sqoop'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown sqoop /user/sqoop'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/oozie'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chmod -R 777 /user/oozie'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -chown -R oozie /user/oozie'
# Do more setup for oozie
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/oozie/share'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/oozie/share/lib'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/oozie/share/lib/hive'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/oozie/share/lib/mapreduce-streaming'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/oozie/share/lib/distcp'
su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -mkdir /user/oozie/share/lib/pig'
# Copy over files from local filesystem to HDFS that oozie might need
if ls /usr/lib/hive/lib/*.jar &> /dev/null; then
  su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -put /usr/lib/hive/lib/*.jar /user/oozie/share/lib/hive'
fi

if ls /usr/lib/hadoop-mapreduce/hadoop-streaming*.jar &> /dev/null; then
  su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -put /usr/lib/hadoop-mapreduce/hadoop-streaming*.jar /user/oozie/share/lib/mapreduce-streaming'
fi

if ls /usr/lib/hadoop-mapreduce/hadoop-distcp*.jar &> /dev/null; then
  su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -put /usr/lib/hadoop-mapreduce/hadoop-distcp*.jar /user/oozie/share/lib/distcp'
fi

if ls /usr/lib/pig/{lib/,}*.jar &> /dev/null; then
  su -s /bin/bash hdfs -c '/usr/bin/hadoop fs -put /usr/lib/pig/{lib/,}*.jar /user/oozie/share/lib/pig'
fi

# Create home directory for the current user if it does not exist
if [ "$1" = "-u" ] ; then
  USER="$2"
  USER=${USER:-$(id -un)}
  EXIST=$(su -s /bin/bash hdfs -c "/usr/bin/hadoop fs -ls /user/${USER}" &> /dev/null; echo $?)
  if [ ! $EXIST -eq 0 ]; then
    su -s /bin/bash hdfs -c "/usr/bin/hadoop fs -mkdir /user/${USER}"
    su -s /bin/bash hdfs -c "/usr/bin/hadoop fs -chmod -R 755 /user/${USER}"
    su -s /bin/bash hdfs -c "/usr/bin/hadoop fs -chown ${USER} /user/${USER}"
  fi
fi
EOL
%files
# default attribues
%defattr(0644, root, root, 0755)

# hdfs config
%config(noreplace) %attr(0644, root, root) "%hdp/etc/hadoop/conf.empty"
%config(noreplace) %attr(0644, root, root) "%hdp/etc/security/limits.d"

# everything else
%attr(0755, root, root) "%hdp/hadoop-hdfs/*"
%attr(0755, root, root) "%hdp/hadoop/*"

#REMOVED %dir %attr(0755, root, root) "/usr/hdp/2.6.1.0-129/hadoop-hdfs/usr"
#REMOVED %dir %attr(0755, root, root) "/usr/hdp/2.6.1.0-129/hadoop-hdfs/usr/lib"
#REMOVED %dir %attr(0755, root, root) "/usr/hdp/2.6.1.0-129/hadoop-hdfs/usr/lib/systemd"
#REMOVED %dir %attr(0755, root, root) "/usr/hdp/2.6.1.0-129/hadoop-hdfs/usr/lib/systemd/system"

#REMOVED %attr(0755, root, root) "/usr/hdp/2.6.1.0-129/hadoop-hdfs/bin/kill-name-node"
#REMOVED %attr(0755, root, root) "/usr/hdp/2.6.1.0-129/hadoop-hdfs/bin/kill-secondary-name-node"
%pre -p /bin/sh
getent group hdfs >/dev/null   || groupadd -r hdfs
getent passwd hdfs >/dev/null || /usr/sbin/useradd --comment "Hadoop HDFS" --shell /bin/bash -M -r -g hdfs -G hadoop --home /var/lib/hadoop-hdfs hdfs

if [[ ! -e "/var/log/hadoop-hdfs" ]]; then
    /usr/bin/install -d -o hdfs -g hadoop -m 0775  /var/log/hadoop-hdfs
fi

if [[ ! -e "/var/run/hadoop-hdfs" ]]; then
    /usr/bin/install -d -o hdfs -g hadoop -m 0775  /var/run/hadoop-hdfs
fi

if [[ ! -e "/var/lib/hadoop-hdfs" ]]; then
    /usr/bin/install -d -o hdfs -g hadoop -m 0755  /var/lib/hadoop-hdfs
fi

if [[ ! -e "/var/lib/hadoop-hdfs/cache" ]]; then
    /usr/bin/install -d -o hdfs -g hadoop -m 1777  /var/lib/hadoop-hdfs/cache
fi
%changelog
