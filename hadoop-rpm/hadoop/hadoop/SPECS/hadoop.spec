# rpmrebuild autogenerated specfile

AutoProv: no
%undefine __find_provides
AutoReq: no
%undefine __find_requires
# Do not try autogenerate prereq/conflicts/obsoletes and check files
%undefine __check_files
%undefine __find_prereq
%undefine __find_conflicts
%undefine __find_obsoletes
# Be sure buildpolicy set to do nothing
%define __spec_install_post %{nil}
# Something that need for rpm-4.1
%define _missing_doc_files_terminate_build 0

# HDP specific parameters
%define hdp_version 2.6.1.0
%define hadoop_src /root/hadoop
%define hadoop_version 3.0.2-SNAPSHOT

BuildRoot:     /root/hadoop/hadoop-rpm/buildroot
BuildArch:     x86_64
Name:          hadoop_2_6_1_0_129
Version:       2.7.3.2.6.1.0
Release:       129
License:       Apache License v2.0 
Group:         Development/Libraries
Summary:       Hadoop is a software platform for processing vast amounts of data


URL:           http://hadoop.apache.org/core/







Provides:      libhadoop.so.1.0.0()(64bit)  
Provides:      libsnappy.so.1()(64bit)  
Provides:      hadoop_2_6_1_0_129 = 2.7.3.2.6.1.0-129
Provides:      hadoop_2_6_1_0_129(x86-64) = 2.7.3.2.6.1.0-129
Requires:      coreutils  
Requires:      /usr/sbin/useradd  
Requires:      /usr/sbin/usermod  
Requires:      /sbin/chkconfig  
Requires:      /sbin/service  
Requires:      zookeeper_2_6_1_0_129 >= 3.4.0
Requires:      ranger_2_6_1_0_129-hdfs-plugin  
Requires:      ranger_2_6_1_0_129-yarn-plugin  
Requires:      hdp-select >= 2.6.1.0-129
Requires:      spark_2_6_1_0_129-yarn-shuffle  
Requires:      spark2_2_6_1_0_129-yarn-shuffle  
Requires:      psmisc  
Requires:      nc  
Requires:      sh-utils  
Requires:      redhat-lsb-core  
Requires:      /bin/sh  
Requires:      /bin/sh  
#Requires:      rpmlib(FileDigests) <= 4.6.0-1
#Requires:      rpmlib(PayloadFilesHavePrefix) <= 4.0-1
#Requires:      rpmlib(CompressedFileNames) <= 3.0.4-1
#Requires:      rpmlib(PayloadIsXz) <= 5.2-1
#suggest
#enhance
%description
Hadoop is a software platform that lets one easily write and
run applications that process vast amounts of data.

Here's what makes Hadoop especially useful:
* Scalable: Hadoop can reliably store and process petabytes.
* Economical: It distributes the data and processing across clusters
              of commonly available computers. These clusters can number
              into the thousands of nodes.
* Efficient: By distributing the data, Hadoop can process it in parallel
             on the nodes where the data is located. This makes it
             extremely rapid.
* Reliable: Hadoop automatically maintains multiple copies of data and
            automatically redeploys computing tasks based on failures.

Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
MapReduce divides applications into many small blocks of work. HDFS creates
multiple replicas of data blocks for reliability, placing them on compute
nodes around the cluster. MapReduce can then process the data where it is
located.
%prep -p /bin/sh
# make sure that the current build dir is configured under /usr/hdp

echo "Build root: $RPM_BUILD_ROOT"
echo "HDP version: %{hdp_version}"
echo "Release: %{release}"
echo "Hadoop src: %{hadoop_src}"
echo "Hadoop version: %{hadoop_version}"

%define hdp_build %hdp_version-%release
%define hdp /usr/hdp/%hdp_version-%release
%define jar_version %{hadoop_version}.%{hdp_build}
HADOOP_DIST=%hadoop_src/hadoop-dist/target/hadoop-%hadoop_version

if [ ! -d "$RPM_BUILD_ROOT" ]; then
  echo "Build root $RPM_BUILD_ROOT doesn't exist"
  mkdir $RPM_BUILD_ROOT
fi

if [ ! -d "$HADOOP_DIST" ]; then
  echo "Hadoop distribution source doesn't exist. Make sure that a version of Hadoop is built from source and is available under hadoop-dist module at $HADOOP_DIST"
  exit -1;
fi

# cleanup everything before we start
rm -rf $RPM_BUILD_ROOT/*

# create a current HDP dir
mkdir -p $RPM_BUILD_ROOT%{hdp}

# copy required hadoop only files from HADOOP_SRC to BUILDROOT
OUT=$RPM_BUILD_ROOT%hdp
DIST=$HADOOP_DIST
SRC=%hadoop_src

# hadoop tab completion
mkdir -p $OUT/etc/bash_completion.d
cp -f $SRC/hadoop-common-project/hadoop-common/src/contrib/bash-tab-completion/hadoop.sh $OUT/etc/bash_completion.d/hadoop

# hadoop default environment variables
mkdir -p $OUT/etc/default
cat > $OUT/etc/default/hadoop <<"EOL"
export HADOOP_HOME_WARN_SUPPRESS=true
export HADOOP_HOME=%{hdp}/hadoop
export HADOOP_PREFIX=%{hdp}/hadoop

export HADOOP_LIBEXEC_DIR=%{hdp}/hadoop/libexec
export HADOOP_CONF_DIR=/etc/hadoop/conf

export HADOOP_COMMON_HOME=%{hdp}/hadoop
export HADOOP_HDFS_HOME=%{hdp}/hadoop-hdfs
export HADOOP_MAPRED_HOME=%{hdp}/hadoop-mapreduce
export HADOOP_YARN_HOME=%{hdp}/hadoop-yarn
export JSVC_HOME=%{hdp}/usr/libexec/bigtop-utils
EOL

# hadoop empty conf
CONF_EMPTY=$OUT/etc/hadoop/conf.empty
mkdir -p $CONF_EMPTY
declare -a ff=(
  configuration.xsl 
  core-site.xml hadoop-env.cmd hadoop-env.sh 
  hadoop-metrics2.properties hadoop-policy.xml 
  kms-acls.xml kms-env.sh kms-log4j.properties kms-site.xml
  log4j.properties workers yarn-env.cmd
  ssl-client.xml.example ssl-server.xml.example
)
for f in ${ff[@]}; do
  cp $DIST/etc/hadoop/$f $CONF_EMPTY
done
cp $DIST/etc/hadoop/workers $CONF_EMPTY/slaves

# hadoop etc config (the dir where various RPMs will place their ACTUAL config into)
ETC_CONF=$RPM_BUILD_ROOT/etc/hadoop/%hdp_build/0
mkdir -p $ETC_CONF

# hadoop 
mkdir -p $OUT/hadoop
# symlink to etc/config
ln -s /etc/hadoop/%hdp_build/0 $OUT/hadoop/conf

# hadoop common jars
declare -A cj=(
  [hadoop-common]=share/hadoop/common/hadoop-common-%{hadoop_version}.jar
  [hadoop-nfs]=share/hadoop/common/hadoop-nfs-%{hadoop_version}.jar
  [hadoop-annotations]=share/hadoop/common/lib/hadoop-annotations-%{hadoop_version}.jar
  [hadoop-auth]=share/hadoop/common/lib/hadoop-auth-%{hadoop_version}.jar
  [hadoop-aws]=share/hadoop/tools/lib/hadoop-aws-%{hadoop_version}.jar
  [hadoop-azure]=share/hadoop/tools/lib/hadoop-azure-%{hadoop_version}.jar
  [hadoop-azure-datalake]=share/hadoop/tools/lib/hadoop-azure-datalake-%{hadoop_version}.jar
)
pushd $OUT/hadoop
for f in "${!cj[@]}"; do
  p=${cj[${f}]}
  cp $DIST/$p $f-%{jar_version}.jar
  ln -s $f-%{jar_version}.jar $f.jar
done
popd

#hadoop-common-2.7.3.2.6.1.0-129-tests.jar"
#hadoop-common-tests.jar"

# hadoop lib
cp -r $DIST/share/hadoop/common/lib $OUT/hadoop

# hadoop bin
mkdir -p $OUT/hadoop/bin
cp $DIST/bin/hadoop $OUT/hadoop/bin/hadoop.distro

# setup HDP specific hadoop shell
cat > $OUT/hadoop/bin/hadoop <<"EOL"
#!/bin/bash

export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"

exec %{hdp}/hadoop/bin/hadoop.distro "$@"
EOL

# setup HDP specific hdfs shell
cat > $OUT/hadoop/bin/hdfs <<"EOL"
#!/bin/bash

export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"

exec %{hdp}/hadoop-hdfs/bin/hdfs.distro "$@"
EOL

## setup HDP specific yarn shell
#cat > $OUT/hadoop/bin/yarn <<"EOL"
##!/bin/bash
#
#export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
#export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
#export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
#export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
#export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
#export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"
#export YARN_OPTS="${YARN_OPTS} -Dhdp.version=${HDP_VERSION}"
#
#exec %{hdp}/hadoop-yarn/bin/yarn.distro "$@"
#EOL

# setup HDP specific mapred shell
#cat > $OUT/hadoop/bin/mapred <<"EOL"
##!/bin/bash
#
#export HADOOP_HOME=${HADOOP_HOME:-%{hdp}/hadoop}
#export HADOOP_MAPRED_HOME=${HADOOP_MAPRED_HOME:-%{hdp}/hadoop-mapreduce}
#export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-%{hdp}/hadoop-yarn}
#export HADOOP_LIBEXEC_DIR=${HADOOP_HOME}/libexec
#export HDP_VERSION=${HDP_VERSION:-%{hdp_build}}
#export HADOOP_OPTS="${HADOOP_OPTS} -Dhdp.version=${HDP_VERSION}"
#export YARN_OPTS="${YARN_OPTS} -Dhdp.version=${HDP_VERSION}"
#
#exec %{hdp}/hadoop-mapreduce/bin/marped.distro "$@"
#EOL

# hadoop etc
mkdir -p $OUT/hadoop/etc
pushd $OUT/hadoop/etc; ln -s ../../hadoop/conf hadoop; popd

# hadoop lib/native
cp -r $DIST/lib/native $OUT/hadoop/lib

# hadoop libexec
mkdir -p $OUT/hadoop/libexec
cp $DIST/libexec/hadoop-config.cmd $OUT/hadoop/libexec
cp $DIST/libexec/hadoop-config.sh $OUT/hadoop/libexec
cp $DIST/libexec/hadoop-functions.sh $OUT/hadoop/libexec
cp -r $DIST/libexec/shellprofile.d $OUT/hadoop/libexec
cat >> $OUT/hadoop/libexec/hadoop-config.sh <<"EOL"

if [[ -z ${IS_HIVE2} && -d "%{hdp}/tez" ]]; then
  export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:%{hdp}/tez/*:%{hdp}/tez/lib/*:%{hdp}/tez/conf
  export CLASSPATH=${CLASSPATH}:%{hdp}/tez/*:%{hdp}/tez/lib/*:%{hdp}/tez/conf
fi

#Adding Ranger check to the hadoop-config.sh
[ -f "%{hdp}/hadoop/conf/set-hdfs-plugin-env.sh" ] && . "%{hdp}/hadoop/conf/set-hdfs-plugin-env.sh" ]

EOL
# hadoop libexec/hadoop-layout.sh
cat > $OUT/hadoop/libexec/hadoop-layout.sh <<"EOL"
HADOOP_COMMON_DIR="./"
HADOOP_COMMON_LIB_JARS_DIR="lib"
HADOOP_COMMON_LIB_NATIVE_DIR="lib/native"
HDFS_DIR="./"
HDFS_LIB_JARS_DIR="lib"
YARN_DIR="./"
YARN_LIB_JARS_DIR="lib"
MAPRED_DIR="./"
MAPRED_LIB_JARS_DIR="lib"

HADOOP_LIBEXEC_DIR=%{hdp}/hadoop/libexec
HADOOP_CONF_DIR=%{hdp}/hadoop/conf
HADOOP_COMMON_HOME=%{hdp}/hadoop
HADOOP_HDFS_HOME=%{hdp}/hadoop-hdfs
HADOOP_MAPRED_HOME=%{hdp}/hadoop-mapreduce
HADOOP_YARN_HOME=%{hdp}/hadoop-yarn
EOL

# hadoop sbin
mkdir -p $OUT/hadoop/sbin
#_hadoop-daemon.sh with predefined HADOOP_HOME
cp $DIST/sbin/hadoop-daemon.sh $OUT/hadoop/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/hadoop/sbin/hadoop-daemon.sh 
#_hadoop-daemons.sh with predefined HADOOP_HOME
cp $DIST/sbin/hadoop-daemons.sh $OUT/hadoop/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/hadoop/sbin/hadoop-daemons.sh 
#_workers.sh and slaves.sh with predefined HADOOP_HOME
cp $DIST/sbin/workers.sh $OUT/hadoop/sbin
sed -i 's/#!\/usr\/bin\/env bash/#!\/usr\/bin\/env bash\n\nexport HADOOP_HOME=\/usr\/hdp\/%{hdp_build}\/hadoop/g' $OUT/hadoop/sbin/workers.sh 
cp $OUT/hadoop/sbin/workers.sh $OUT/hadoop/sbin/slaves.sh

# mapreduce.tar.gz
# extract the files, then rename the top-level dir from hadoop-${version} to hadoop
pushd $DIST/../
rm -rf /tmp/mapreduce
mkdir /tmp/mapreduce
tar xvf hadoop-%{hadoop_version}.tar.gz -C /tmp/mapreduce
mv /tmp/mapreduce/hadoop-%{hadoop_version} /tmp/mapreduce/hadoop
pushd /tmp/mapreduce
tar zcvf $OUT/hadoop/mapreduce.tar.gz hadoop
popd

%files
# default attribues
%defattr(0644, root, root, 0755)

# hadoop config
%config(noreplace) "%hdp/etc/default/hadoop"
%config(noreplace) %attr(0644, root, root) "%hdp/etc/hadoop/conf.empty"
%attr(0644, root, root) "%hdp/etc/bash_completion.d/hadoop"

# everything else
%attr(0755, root, root) "%hdp/hadoop/*"

%pre -p /bin/sh
getent group hadoop >/dev/null || groupadd -r hadoop
%post -p /bin/sh
if [ !  -e "/etc/hadoop/conf" ]; then
    rm -f /etc/hadoop/conf
    mkdir -p /etc/hadoop/conf
    cp -rp %hdp/etc/hadoop/conf.empty/* /etc/hadoop/conf
fi
/usr/bin/hdp-select --rpm-mode set hadoop-client %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-hdfs-datanode %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-hdfs-journalnode %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-hdfs-nfs3 %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-hdfs-namenode %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-hdfs-portmap %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-hdfs-secondarynamenode %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-hdfs-zkfc %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-mapreduce-historyserver %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-yarn-resourcemanager %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-yarn-nodemanager %hdp_build
/usr/bin/hdp-select --rpm-mode set hadoop-yarn-timelineserver %hdp_build
%changelog
